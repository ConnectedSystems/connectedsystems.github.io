# Announcement: SALib v1.4 development pre-release

On behalf of the SALib development team I'm pleased to announce that a new
development release is available via PyPI. Try it out with:

```bash
pip install --pre salib==1.4.0.dev2
```

Report any issues that may occur over at [GitHub](https://github.com/SALib/SALib/issues/new).

Some upcoming feature highlights:

- Improved documentation and an FAQ
- Better plotting support
- Expanded support for different sampling distributions
- FAST and RBD-FAST methods now estimate confidence intervals as well
- Uniform `print_to_console` using DataFrames

Features considered experimental for this release include:

- implementation of the High Dimensional Model Reduction method (thanks to @sahin-abdullah)
- The Lake Problem (given in [Hadka et al., 2015](https://dx.doi.org/10.1016/j.envsoft.2015.07.014); [Kwakkel et al., 2017](https://dx.doi.org/10.1016/j.envsoft.2017.06.054))
- A new approach to using SALib, based on method chaining
- Out-of-the-box parallel model evaluation

I'll be giving examples of the last two in this post.

First the set up:

```python
import numpy as np
import time

from SALib.test_functions import Ishigami
from SALib import ProblemSpec


# Problem specification for the Ishigami-Homma test function
sp = ProblemSpec({
    'names': ['x1', 'x2', 'x3'],
    'bounds': [[-np.pi, np.pi]*3],
    'outputs': ['Y']
})
```

For this example we assign the `ProblemSpec` object to `sp`, for "[S]ALib [P]roblem" or "[sp]ecification". Of course, this variable can be named anything.

Compare the above to the original approach:

```python
problem = {
    'names': ['x1', 'x2', 'x3'],
    'num_vars': 3,
    'bounds': [[-np.pi, np.pi]*3]
}
```

At first glance, not much has changed. 

The number of variables (`num_vars`) is now inferred from the number of input 
names. The SALib `ProblemSpec` accepts `outputs` to be optionally defined as well, which comes 
in handy when it is time to display results.

That said, there are several benefits to using the new approach.

One is readability:

```python
# New approach
(sp.sample_saltelli(1000)
   .evaluate(Ishigami.evaluate)
   .analyze_sobol(print_to_console=True))
```

The outer parenthesis `()` is optional. Without it, it looks like this (a little clunky in my opinion):

```python
sp.sample_saltelli(1000)\
  .evaluate(Ishigami.evaluate)\
  .analyze_sobol(print_to_console=True)
```

The backslashes (`\`) tells Python "pretend these are all on the same line", and so using the outer parenthesis is a stylistic choice.

Unlike the older approach, there is no requirement to import the specific methods (e.g. `saltelli`). Both the original procedural approach and the new approach are supported.


```python
# Original approach 
from SALib.sample import saltelli
from SALib.analyze import sobol

X = saltelli.sample(problem, 1000)
Y = Ishigami.evaluate(X)
Si = sobol.analyze(problem, Y, print_to_console=True)

# The ProblemSpec is compatible with the procedural approach too
# Note we're passing in `sp`, not `problem`!
X = saltelli.sample(sp, 1000)
Y = Ishigami.evaluate(X)
Si = sobol.analyze(sp, Y, print_to_console=True)
```

In comparing the two approaches, the newer method no longer requires the user to track, and pass in, `problem`, `X`, or `Y` parameters as these are handled automatically.

Another advantage is that the available sampling and analysis methods are made *user discoverable*. The full list of available methods can be found via tab-completion in a REPL or Jupyter Notebook.

![](/assets/images/salib_sp_discoverability.gif "gif showcasing discoverability of available sampling and analysis methods")

Generic `sample` and `analyze` methods are also available, and can use any function that accepts the SALib `ProblemSpec` as its first parameter. The `analyze` method should also accept a `Y` parameter (and `X` as well if the method requires it):

```python
from SALib.sample import saltelli
from SALib.analyze import sobol

(sp.sample(saltelli.sample, 1000)
   .evaluate(Ishigami.evaluate)
   .analyze(sobol.analyze, print_to_console=True))
```

Previously obtained samples and results can be attached like so:

```python
sp.samples = previous_samples
sp.results = pre_existing_model_outputs
sp.analyze_sobol()
```

The aim of all of this is to simplify the workflow involved in SA.

With that in mind, the `evaluate` method has a parallel counterpart: `evaluate_parallel`.
Model runs conducted with this method will auto-magically divide the parameter sets to the available (or specified number of) processors.

![gif showcasing parallel evaluation of target model](/assets/images/salib_sp_parallel_evaluation.gif "gif showcasing parallel evaluation of target model")

Relevant code snippet:

```python
(sp.sample_saltelli(25000)
   .evaluate_parallel(Ishigami.evaluate, nprocs=4)
   .analyze_sobol())

print(sp)

# Can also plot results
sp.plot()
```

As noted by the warning shown in the example above, this is an 
**experimental feature** for now, and likely only useful for long-running 
models given the overhead involved in managing distributed runs.

A further benefit, albeit one which regular users won't experience, is that it will be
much easier to develop and maintain future planned improvements.

That's all for now!

Once again, try it out with

```bash
pip install --pre salib==1.4.0.dev2
```

Any and all feedback is appreciated.
